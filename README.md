# üß† Multimodal Hate Speech Detection App

This is a Streamlit-based application for detecting hate speech across **text, documents, and audio/video files**. It leverages **OpenAI GPT-3.5** for classification and explanation, and **Whisper** for audio transcription. Optional multimodal analysis integrates video frame extraction and image-text alignment.

---

## üîç Key Features

- ‚úçÔ∏è **Text Input**: Detect hate or non-hate speech in typed content  
- üìÑ **Document Upload**: Analyze hate speech in PDF or Word files  
- üéß **Audio/Video Upload**: Automatically transcribe and analyze audio or video content  
- üåê **Multilingual Support**: Auto-detects language and works across various languages  
- ü§ñ **OpenAI GPT Classification**: Provides hate/non-hate label and short explanation  
- üîà **Whisper Transcription**: Converts speech to text for analysis  
- üñºÔ∏è **(Optional) Video Frame Analysis**: Supports image-text multimodal classification if a ResNet model is provided  
- üì• **Export Support**: Download CSV results of video-text analysis  

---

## üñ•Ô∏è How to Run Locally

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

> Make sure you have `ffmpeg` installed (needed for Whisper/video processing).

### 3. Set Up Environment Variables

Create a `.env` file in the root directory and add your OpenAI key:

```env
OPENAI_API_KEY=your-openai-key
```

Alternatively, you can hardcode it (as in the script) or modify the script to load from `.env`.

### 4. Run the Application

```bash
streamlit run streamlit_app_final.py
```

---

## üìÅ Project Structure

- `streamlit_app_final.py` ‚Äì Main app logic (text, audio, video handling)  
- `pipeline.py` ‚Äì (Optional) Utility functions for frame extraction, multimodal analysis  
- `requirements.txt` ‚Äì Python dependencies  
- `.env` ‚Äì API key config (optional)  
- `harmful_content_classifier_resnet50.pth` ‚Äì (Optional) Pretrained image classifier model (if multimodal)  

---

## üß† Model Logic

### Text Classification (GPT)
OpenAI GPT-3.5 is used with the following prompt:

```
You are a hate speech detection assistant. Analyze the given text.
First, classify it strictly as either 'hate' or 'non-hate'.
Then provide a brief explanation (1‚Äì2 sentences) why it is classified that way.
Format your reply as:
Label: [hate/non-hate]
Explanation: [your reason]
```

### Audio Transcription
Whisper's `"base"` model is used to convert audio/video to text for further analysis.

### Optional: Multimodal Analysis
If enabled, the app extracts frames from uploaded videos and runs image-text analysis (requires a custom model and pipeline).

---

## üì¶ Output

- Text and document results are shown on screen with GPT‚Äôs explanation  
- Video analysis results are shown as a dataframe and downloadable as CSV